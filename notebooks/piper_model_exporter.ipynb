{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/subspecs/Cocaine/blob/master/notebooks/piper_model_exporter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOL-kjplZYEU"
      },
      "source": [
        "# <font color=\"ffc800\"> **[Piper](https://github.com/rhasspy/piper) model exporter.**\n",
        "## ![Piper logo](https://contribute.rhasspy.org/img/logo.png)\n",
        "---\n",
        "\n",
        "* Notebook created by: [rmcpantoja](http://github.com/rmcpantoja)\n",
        "* Collaborator: [Xx_Nessu_xX](http://github.com/XxNessuxX)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "FfMKr8v2RVOm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a910664-d4a1-4b5f-e1f7-62c214251827"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[93mInstalling...\n",
            "fatal: destination path 'piper' already exists and is not an empty directory.\n",
            "/content/piper/src/python\n",
            "Requirement already satisfied: pip==24.0 in /usr/local/lib/python3.11/dist-packages (24.0)\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.0.2 requires torch==2.0.1, but you have torch 2.3.1 which is incompatible.\n",
            "torchtext 0.15.2 requires torch==2.0.1, but you have torch 2.3.1 which is incompatible.\n",
            "torchdata 0.6.1 requires torch==2.0.1, but you have torch 2.3.1 which is incompatible.\n",
            "torchvision 0.15.2 requires torch==2.0.1, but you have torch 2.3.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "espeak-ng is already the newest version (1.50+dfsg-10ubuntu0.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n",
            "\u001b[93mDone!\n"
          ]
        }
      ],
      "source": [
        "#@markdown # <font color=\"ffc800\"> **Install software.** üì¶\n",
        "#@markdown ---\n",
        "\n",
        "print(\"\\033[93mInstalling...\")\n",
        "!git clone -q https://github.com/rhasspy/piper\n",
        "%cd /content/piper/src/python\n",
        "!pip install pip==24.0\n",
        "!pip install -q cython>=0.29.0 librosa>=0.9.2 numpy>=1.19.0 pytorch-lightning~=1.9.0 torch~=2.3.0\n",
        "!pip install -q onnx onnxruntime-gpu\n",
        "!bash build_monotonic_align.sh\n",
        "!apt-get install espeak-ng\n",
        "!pip install -q torchtext==0.15.2\n",
        "!pip install -q torchaudio==2.0.2 torchmetrics==0.11.4\n",
        "#!pip install torch -U\n",
        "#!pip install --upgrade gdown\n",
        "\n",
        "print(\"\\033[93mDone!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "cellView": "form",
        "id": "PqcoBb26V5xA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 734
        },
        "outputId": "538c4b23-c28e-43c6-eaac-c8e464b9f3a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/piper/src/python\n",
            "\u001b[93mDownloading model and his config...\n",
            "2025-01-25 11:55:07.014304: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-01-25 11:55:07.345040: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-01-25 11:55:07.434305: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-01-25 11:55:07.992018: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-01-25 11:55:10.534361: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
            "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
            "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
            "Removing weight norm...\n",
            "/content/piper/src/python/piper_train/vits/attentions.py:235: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  t_s == t_t\n",
            "/content/piper/src/python/piper_train/vits/attentions.py:295: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  pad_length = max(length - (self.window_size + 1), 0)\n",
            "/content/piper/src/python/piper_train/vits/attentions.py:296: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  slice_start_position = max((self.window_size + 1) - length, 0)\n",
            "/content/piper/src/python/piper_train/vits/attentions.py:298: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if pad_length > 0:\n",
            "/content/piper/src/python/piper_train/vits/transforms.py:174: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  assert (discriminant >= 0).all(), discriminant\n",
            "/usr/local/lib/python3.11/dist-packages/torch/onnx/_internal/jit_utils.py:306: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at ../torch/csrc/jit/passes/onnx/constant_fold.cpp:179.)\n",
            "  _C._jit_pass_onnx_node_shape_type_inference(node, params_dict, opset_version)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/onnx/utils.py:689: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at ../torch/csrc/jit/passes/onnx/constant_fold.cpp:179.)\n",
            "  _C._jit_pass_onnx_graph_shape_type_inference(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/onnx/utils.py:1186: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at ../torch/csrc/jit/passes/onnx/constant_fold.cpp:179.)\n",
            "  _C._jit_pass_onnx_graph_shape_type_inference(\n",
            "============= Diagnostic Run torch.onnx.export version 2.0.1+cu117 =============\n",
            "verbose: False, log level: Level.ERROR\n",
            "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
            "\n",
            "INFO:piper_train.export_onnx:Exported model to /content/project/voice-en_US-yui-medium/en_US-yui-medium.onnx\n",
            "\u001b[93mCompressing...\n",
            "./\n",
            "./en_US-yui-medium.onnx.json\n",
            "./en_US-yui-medium.onnx\n",
            "\u001b[93mDone!\n"
          ]
        }
      ],
      "source": [
        "#@markdown # <font color=\"ffc800\"> **Voice package generation section.** üó£Ô∏è\n",
        "#@markdown ---\n",
        "%cd /content/piper/src/python\n",
        "import os\n",
        "import json\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "import json\n",
        "from google.colab import output\n",
        "guideurl = \"https://github.com/rmcpantoja/piper/blob/master/notebooks/wav/en\"\n",
        "#@markdown #### *Download:*\n",
        "#@markdown **Drive ID or direct download link of the model in another cloud:**\n",
        "model_id = \"https://drive.google.com/file/d/1-GMshutkNwxI7ZgVADxS8cV_FsEE6BD9/view?usp=sharing\" #@param {type:\"string\"}\n",
        "#@markdown **Drive ID or direct download link of the config.json file:**\n",
        "config_id = \"https://drive.google.com/file/d/1-CE84QOoR6haOAxEyKNz-tFO9dkTLzJP/view?usp=sharing\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown #### *Creation process:*\n",
        "#@markdown **Choose the language code (iso639-1 format):**\n",
        "\n",
        "#@markdown You can see a list of language codes and names [here](https://www.loc.gov/standards/iso639-2/php/English_list.php).\n",
        "\n",
        "language = \"en_US\" #@param [\"ar_JO\", \"ca_ES\", \"cs_CZ\", \"da_DK\", \"de_DE\", \"el_GR\", \"en_GB\", \"en_US\", \"es_ES\", \"es_LA\", \"fi_FI\", \"fr_FR\", \"grc\", \"hu_GU\", \"is_IS\", \"it_IT\", \"kk_KZ\", \"ka_GE\", \"lb_LU\", \"nb\", \"ne\", \"nl_BE\", \"no_NO\", \"pl_PL\", \"pt_BR\", \"pt_PT\", \"ro_RO\", \"ru_RU\", \"sk_SK\", \"sr\", \"sv_SE\", \"sw_CD\", \"tr_TR\", \"uk_UA\", \"vi_VN\", \"zh_CN\"]\n",
        "voice_name = \"Yui\" #@param {type:\"string\"}\n",
        "voice_name = voice_name.lower()\n",
        "quality = \"medium\" #@param [\"high\", \"low\", \"medium\", \"x-low\"]\n",
        "#@markdown **Do you want to write a model card?** *(Optional.)*\n",
        "write_model_card = False #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown **Do you want this voice to have a faster response speed?**\n",
        "streaming = False #@param {type:\"boolean\"}\n",
        "\n",
        "def start_process(streaming):\n",
        "    if not os.path.exists(\"/content/project/model.ckpt\"):\n",
        "        raise Exception(\"Could not download model! make sure the file is shareable to everyone\")\n",
        "    output.eval_js(f'new Audio(\"{guideurl}/starting.wav?raw=true\").play()')\n",
        "    if not streaming:\n",
        "        !python -m piper_train.export_onnx \"/content/project/model.ckpt\" \"{export_voice_path}/{export_voice_name}.onnx\"\n",
        "    else:\n",
        "        !python -m piper_train.export_onnx_streaming \"/content/project/model.ckpt\" \"{export_voice_path}\"\n",
        "    print(\"\\033[93mCompressing...\")\n",
        "    !tar -czvf \"{packages_path}/{export_voice_name}.tar.gz\" -C \"{export_voice_path}\" .\n",
        "    output.eval_js(f'new Audio(\"{guideurl}/success.wav?raw=true\").play()')\n",
        "    print(\"\\033[93mDone!\")\n",
        "\n",
        "if not streaming:\n",
        "    export_voice_name = f\"{language}-{voice_name}-{quality}\"\n",
        "else:\n",
        "    export_voice_name = f\"{language}-{voice_name}+RT-{quality}\"\n",
        "export_voice_path = \"/content/project/voice-\"+export_voice_name\n",
        "packages_path = \"/content/project/packages\"\n",
        "if not os.path.exists(export_voice_path):\n",
        "    os.makedirs(export_voice_path)\n",
        "if not os.path.exists(packages_path):\n",
        "    os.makedirs(packages_path)\n",
        "print(\"\\033[93mDownloading model and his config...\")\n",
        "if model_id.startswith(\"1\"):\n",
        "    !gdown -q \"{model_id}\" -O /content/project/model.ckpt\n",
        "elif model_id.startswith(\"https://drive.google.com/file/d/\"):\n",
        "    !gdown -q \"{model_id}\" -O \"/content/project/model.ckpt\" --fuzzy\n",
        "else:\n",
        "    !wget \"{model_id}\" -O \"/content/project/model.ckpt\"\n",
        "if config_id.startswith(\"1\"):\n",
        "    !gdown -q \"{config_id}\" -O \"{export_voice_path}/{export_voice_name}.onnx.json\"\n",
        "elif config_id.startswith(\"https://drive.google.com/file/d/\"):\n",
        "    !gdown -q \"{config_id}\" -O \"{export_voice_path}/{export_voice_name}.onnx.json\" --fuzzy\n",
        "else:\n",
        "    !wget \"{config_id}\" -O \"{export_voice_path}/{export_voice_name}.onnx.json\"\n",
        "\n",
        "if os.path.exists(f\"{export_voice_path}/{export_voice_name}.onnx.json\") and streaming:\n",
        "    with open(f\"{export_voice_path}/{export_voice_name}.onnx.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "        tmp = f.read()\n",
        "    new_config = json.loads(tmp)\n",
        "    new_config[\"streaming\"] = True\n",
        "    new_config[\"key\"] = export_voice_name\n",
        "\n",
        "    with open(f\"{export_voice_path}/{export_voice_name}.onnx.json\", \"w\", encoding=\"utf-8\") as f_new:\n",
        "        json.dump(new_config, f_new, indent=4)\n",
        "\n",
        "if write_model_card:\n",
        "    with open(f\"{export_voice_path}/{export_voice_name}.onnx.json\", \"r\") as file:\n",
        "        config = json.load(file)\n",
        "    sample_rate = config[\"audio\"][\"sample_rate\"]\n",
        "    num_speakers = config[\"num_speakers\"]\n",
        "    output.eval_js(f'new Audio(\"{guideurl}/waiting.wav?raw=true\").play()')\n",
        "    text_area = widgets.Textarea(\n",
        "        description = \"fill in this following template and press start to generate the voice package\",\n",
        "        value=f'# Model card for {voice_name} ({quality})\\n\\n* Language: {language} (normaliced)\\n* Speakers: {num_speakers}\\n* Quality: {quality}\\n* Samplerate: {sample_rate}Hz\\n\\n## Dataset\\n\\n* URL: \\n* License: \\n\\n## Training\\n\\nTrained from scratch.\\nOr finetuned from: ',\n",
        "        layout=widgets.Layout(width='500px', height='200px')\n",
        "    )\n",
        "    button = widgets.Button(description='Start')\n",
        "\n",
        "    def create_model_card(button):\n",
        "        model_card_text = text_area.value.strip()\n",
        "        with open(f'{export_voice_path}/MODEL_CARD', 'w') as file:\n",
        "            file.write(model_card_text)\n",
        "        text_area.close()\n",
        "        button.close()\n",
        "        output.clear()\n",
        "        start_process(streaming)\n",
        "\n",
        "    button.on_click(create_model_card)\n",
        "\n",
        "    display(text_area, button)\n",
        "else:\n",
        "    start_process(streaming)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Hu3V9CJeWc4Y"
      },
      "outputs": [],
      "source": [
        "#@markdown # <font color=\"ffc800\"> **Download/export your generated voice package.** üì•\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown #### *How do you want to export your model?*\n",
        "export_mode = \"upload it to my Google Drive\" #@param [\"Download the voice package on my device (may take some time)\", \"upload it to my Google Drive\"]\n",
        "print(\"\\033[93mExporting package...\")\n",
        "if export_mode == \"Download the voice package on my device (may take some time)\":\n",
        "    from google.colab import files\n",
        "    files.download(f\"{packages_path}/{export_voice_name}.tar.gz\")\n",
        "    msg = \"Please wait a moment while the package is being downloaded.\"\n",
        "else:\n",
        "    voicepacks_folder = \"/content/drive/MyDrive/piper voice packages\"\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    if not os.path.exists(voicepacks_folder):\n",
        "        os.makedirs(voicepacks_folder)\n",
        "    !cp \"{packages_path}/{export_voice_name}.tar.gz\" \"{voicepacks_folder}\"\n",
        "    msg = f\"You can find the generated voice package at: {voicepacks_folder}.\"\n",
        "print(f\"\\033[93mDone! {msg}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRiNBHkeoDbC"
      },
      "source": [
        "# \"*I want to test this model! I don't need anything else anymore?*\"\n",
        "\n",
        "No, this is almost the end! Now you can share your generated package to your friends, upload to a cloud storage and/or test it on:\n",
        "* [The inference notebook](https://colab.research.google.com/github/rmcpantoja/piper/blob/master/notebooks/piper_inference_(ONNX).ipynb)\n",
        "  * run the cells in order for it to work correctly, as well as all the notebooks. Also, the inference notebook will guide you through the process using the enhanced accessibility feature if you wish. It's easy to use. Test it!\n",
        "* Or through the NVDA screen reader!\n",
        "  * Download and install the latest version of the [add-on](https://github.com/mush42/piper-nvda/releases).\n",
        "  * Once the add-on is installed, go to NVDA menu/piper voice manager...\n",
        "  * In the installed voices page, tab until you find the `Install from local file` button, press enter and select the generated package in your downloads.\n",
        "  * Once the package is selected and installed, apply the changes and restart NVDA to update the voice list.\n",
        "* Enjoy your creation!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}